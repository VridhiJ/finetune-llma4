# finetune-llma4
will be used for our major project

There is a granth: 
-Siri bhuvalay
- nxn(probably 27 and something to do with 64 )matrix; each numeral represents an alphabet of a language. One image covers 17 languages(probably) and multiple stories.
Possible outcomes:
- benchmark for LLMs (Multimodal LLMs)
- Coding Algorithm: possible; find a new one
- find new data compression technique: example: entire literature of the world can be compressed; sorted indexing(search and retrieval will be faster)
- multilingual new kind of qr code for multilingual people
- acceleration for google translate 
- bhashini

Devops pipeline
1st job: How to finetune a model 
Llama 4 
Set up a pipeline to finetune a model
free(biggest innovation)
2nd: get the data corpus of world theology (get a copy of siri bhuvalay): digitize it accurately; find itâ€™s references and everything 
23rd: visit india (get all the copies)
Backup plan: engraving the engravings from jain pratimas(crowdsource: make a project for students aged 9-15 where students upload the pics; kids will learn something about their heritage and upload in common jain database); ancient languages which has not been read
3rd job: while we wait for the data, we should get another data for finetuning the model over data we collect beside siri bhuvalay over smaller dataset(image/text)  

Lang chain

